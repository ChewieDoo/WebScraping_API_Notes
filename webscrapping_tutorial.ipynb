{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscrapping Tutorial using BeautifulSoup\n",
    "\n",
    "This notebook includes my notes on the webscrapping tutorial using Beautiful Soup from realpython.com.  The markdown panels are where I jot down my thought processes.  I also included code snippets where I put some of these concepts to use.  The tutorial takes a comprehensive journey through a **web scraping pipeline** from 1. Inspecting the webpage, 2. Understanding DOMs and HTML structures, 3. Connecting to Data Source and Navigating the HTML structures with Beautiful Soup Package.  The webpage we use is a static fake job website where we will be accessing various information on there.  All relevant links are included below.\n",
    "\n",
    "The purpose of this notebook is for me to remember what I have learned on the tutorial and sharing my thought process for other learners.  I must say not everything on this page is up to the best coding practice.  I just started learning to code with Python and I would love it if you can provide feedback whether if you spot any errors, syntax, and give me suggestions to improve my coding etiquette.\n",
    "\n",
    "Shout out to Martin Beuss for making this comprehensive tutorial.  I have learned a lot from it and I hope you do as well!\n",
    "\n",
    "\n",
    "Data source: https://realpython.github.io/fake-jobs/\n",
    "\n",
    "Tutorial link: https://realpython.com/beautiful-soup-web-scraper-python\n",
    "\n",
    "HTML formatter: https://webformatter.com/html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "URL = \"https://realpython.github.io/fake-jobs/\"\n",
    "page = requests.get(URL) # retrieve HTML data and stores it into a Python object\n",
    "\n",
    "print(page.text) # print out .text attribute which looks just like the html source codes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below extracts all the HTML elements relating to the individual job panels.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup # Allows parsing of structured data\n",
    "\n",
    "URL = \"https://realpython.github.io/fake-jobs/\"\n",
    "page = requests.get(URL) \n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\") # Input: HTML content, html.parser is the appropriate Parser for our data\n",
    "\n",
    "results = soup.find(id=\"ResultsContainer\") # Finding specific HTML element of id \"ResultsContainer\"\n",
    "print(results.prettify()) # .prettify() puts all HTML content contained behind a <div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the content is more structured into the hierarchy formate we typically see HTML elements in, it is still long and messy.  \n",
    "\n",
    "### Finding elements by HTML Classes\n",
    "\n",
    "Every job posting on the page is wrapped in ```<div\\>``` with class \"card-content\" (e.g. ```<div class=\"card-content\"\\>```).  The code below allows us to only view job postings by specifying the class (i.e. \"card-content\") that we are intereted in.  It seems that **HTML classes doesn't have to exactly mimic the class content, as long as you just include a \"unique part of it\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_elements = results.find_all(\"div\", class_=\"card-content\")  # Creates an 'iterable' containing all HTML for job postings\n",
    "\n",
    "# Using a for loop to print out all elements of the 'iterable'\n",
    "for job_element in job_elements:\n",
    "    print(job_element.prettify(), end=\"\\n\"*2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output is still very lengthy but a lot neater than the previous output.  We can still trim this down by picking the child elements from each of the job-postings with ```.find()```.  These child elements are refering to the job titles, company, and locations.\n",
    "\n",
    "\n",
    "Each child element is also a BeautifulSoup object so we can use ```.find()``` to locate these elements like the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_element in job_elements:\n",
    "    title_element = job_element.find(\"h2\", class_=\"title\") \n",
    "    company_element = job_element.find(\"h3\", class_=\"company\")\n",
    "    location_element = job_element.find(\"p\", class_=\"location\")\n",
    "    print(title_element.prettify())\n",
    "    print(company_element.prettify())\n",
    "    print(location_element.prettify())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is MUCH more organized.  The codeis showing us the job title, the company, and the location of the job.  However, **there are still HTML elements floating around.**  Our goal is to only extract text information from the webpage.\n",
    "\n",
    "\n",
    "### Extracting Text from HTML elements\n",
    "\n",
    "\n",
    "Adding .text to a Beautiful Soup object returns the text content of the HTML element.  After we have converted it to a text element, we can edit the text however we want using Python functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_element in job_elements:\n",
    "    title_element = job_element.find(\"h2\", class_=\"title\")\n",
    "    company_element = job_element.find(\"h3\", class_=\"company\")\n",
    "    location_element = job_element.find(\"p\", class_=\"location\")\n",
    "    print(title_element.text.strip())\n",
    "    print(company_element.text.strip())\n",
    "    print(location_element.text.strip()) # .strip() gets rid of all the white spaces \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Elements by Class Name and Text Content\n",
    "\n",
    "Say we are only interested in finding developer jobs, we will need to further filter down our findings.  We can do so in the ```.find_all()``` by passing the HTML elements and the specific strings.  For instance, say we want to find all Python jobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "python_jobs = results.find_all(\"h2\", string = \"Python\")\n",
    "print(python_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What? How come no results are showing up? Well that is because the ```string =``` only look for the specified string exactly.  We need to search for strings in a more general sense.\n",
    "\n",
    "### Passing function to a Beautiful Soup Method\n",
    "\n",
    "\n",
    "Here we pass a **anonymous function**  (the lambda function) into the string = arguement, which looks at the text of each ```<h2>``` element, converting it to lowercase, and check whether the substring \"python\" is found anywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "python_jobs = results.find_all(\"h2\", string = lambda text: \"python\" in text.lower())\n",
    "print(len(python_jobs)) # show number of html elements found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are see there are 10 python jobs. (Note: the 10 refers to 10 html elements found which are each of the texts enclosed wthin ```<h2>```)  We can try and use the loop we previously made to extract all the job titles, companies, and locations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_jobs = results.find_all(\n",
    "    \"h2\", string=lambda text: \"python\" in text.lower()\n",
    ")\n",
    "\n",
    "for job_element in python_jobs:\n",
    "    title_element = job_element.find(\"h2\", class_=\"title\")\n",
    "    company_element = job_element.find(\"h3\", class_=\"company\")\n",
    "    location_element = job_element.find(\"p\", class_=\"location\")\n",
    "    print(title_element.text.strip())\n",
    "    print(company_element.text.strip())\n",
    "    print(location_element.text.strip())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an error message ```'AttributeError: 'NoneType' object has no attribute 'text''```.  If we were to see all the elements in python_jobs, we would only find job being included in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_jobs = results.find_all(\"h2\", string = lambda text: \"python\" in text.lower())\n",
    "print(python_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to get through this issue by adding an IF statement condition we are familiar with, where it would only give us the job description if the word \"Python\" is found in the job title element of the HTML.  I set this condition under a loop through the job_elements iterable we created in the previous section.  It seems to work fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Senior Python Developer', 'Payne, Roberts and Davis', 'Stewartbury, AA', 'Software Engineer (Python)', 'Garcia PLC', 'Ericberg, AE', 'Python Programmer (Entry-Level)', 'Moss, Duncan and Allen', 'Port Sara, AE', 'Python Programmer (Entry-Level)', 'Cooper and Sons', 'West Victor, AE', 'Software Developer (Python)', 'Adams-Brewer', 'Brockburgh, AE', 'Python Developer', 'Rivera and Sons', 'East Michaelfort, AA', 'Back-End Web Developer (Python, Django)', 'Stewart-Alexander', 'South Kimberly, AA', 'Back-End Web Developer (Python, Django)', 'Jackson, Ali and Mckee', 'New Elizabethside, AA', 'Python Programmer (Entry-Level)', 'Mathews Inc', 'Robertborough, AP', 'Software Developer (Python)', 'Moreno-Rodriguez', 'Martinezburgh, AE']\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "job_list = []\n",
    "\n",
    "for job_element in job_elements:\n",
    "    title_element = job_element.find(\"h2\", class_=\"title\")\n",
    "    company_element = job_element.find(\"h3\", class_=\"company\")\n",
    "    location_element = job_element.find(\"p\", class_=\"location\")\n",
    "    if \"Python\" in title_element.text.strip(): # Only print out job if it has \"Python\" in the job title\n",
    "        job_list.append(title_element.text.strip())\n",
    "        job_list.append(company_element.text.strip())\n",
    "        job_list.append(location_element.text.strip())\n",
    "\n",
    "print(job_list)\n",
    "print(len(job_list)) # 30 meaning 10 jobs since each job card has 3 parts to it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Parent Elements\n",
    "\n",
    "See the hiearchy of the DOM starting from ```<h2>``` that we are interested in.  Then identify the parent elements associated with it which emcompasses all the elements we need (```<div>``` stands for division, which is used as a container for HTML).  We are interested in the ```<div>``` element with card-content class because it contains not just job title, but alss company and location.  It’s a **3rd level** parent of the ```<h2>``` title element (hint: on visual code studio, count the number of lines that lead you to that divison)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "<div class=\"card\">\n",
    "    <div class=\"card-content\"> \n",
    "      <div class=\"media\">\n",
    "        <div class=\"media-left\">\n",
    "          <figure class=\"image is-48x48\">\n",
    "            <img\n",
    "              src=\"https://files.realpython.com/media/real-python-logo-thumbnail.7f0db70c2ed2.jpg\"\n",
    "              alt=\"Real Python Logo\"\n",
    "            />\n",
    "          </figure>\n",
    "        </div>\n",
    "        <div class=\"media-content\">\n",
    "          <h2 class=\"title is-5\">Senior Python Developer</h2>\n",
    "          <h3 class=\"subtitle is-6 company\">Payne, Roberts and Davis</h3>\n",
    "        </div>\n",
    "      </div>\n",
    "  \n",
    "      <div class=\"content\">\n",
    "        <p class=\"location\">Stewartbury, AA</p>\n",
    "        <p class=\"is-small has-text-grey\">\n",
    "          <time datetime=\"2021-04-08\">2021-04-08</time>\n",
    "        </p>\n",
    "      </div>\n",
    "      <footer class=\"card-footer\">\n",
    "        <a\n",
    "          href=\"https://www.realpython.com\"\n",
    "          target=\"_blank\"\n",
    "          class=\"card-footer-item\"\n",
    "          >Learn</a\n",
    "        >\n",
    "        <a\n",
    "          href=\"https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html\"\n",
    "          target=\"_blank\"\n",
    "          class=\"card-footer-item\"\n",
    "          >Apply</a\n",
    "        >\n",
    "      </footer>\n",
    "    </div>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we know the \"number of nodes\" from the one we were using to filter, we can use .parent to extract information we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all elements with \"python\" in text\n",
    "python_jobs = results.find_all(\n",
    "    \"h2\", string=lambda text: \"python\" in text.lower()\n",
    ") \n",
    "\n",
    "# List comprehension to extract the 3 levels of hiearchy, looping through python jobs\n",
    "python_job_elements = [\n",
    "    h2_element.parent.parent.parent for h2_element in python_jobs # show the 3 levels of hiearchy between <div> content-card and <h2> \n",
    "]\n",
    "\n",
    "# Note that python_job_elements is still there \n",
    "\n",
    "# Print out all the Python related jobs \n",
    "job_list = []\n",
    "\n",
    "for job_element in python_job_elements:\n",
    "    title_element = job_element.find(\"h2\", class_=\"title\")\n",
    "    company_element = job_element.find(\"h3\", class_=\"company\")\n",
    "    location_element = job_element.find(\"p\", class_=\"location\")\n",
    "    job_list.append(title_element.text.strip())\n",
    "    job_list.append(company_element.text.strip())\n",
    "    job_list.append(location_element.text.strip())\n",
    "    print(title_element.text.strip())\n",
    "    print(company_element.text.strip())\n",
    "    print(location_element.text.strip())\n",
    "    print()\n",
    "\n",
    "print(len(job_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see now that our loop loops over the entire the \\<div class=\"card-content\"> elements instead of just the \\<h2> elements.  All this was because we identified that \\<div class=\"card-content\"> is the HTML element that contains all the HTML elements we are interested in.\n",
    "\n",
    "On a side note, it also seems that the number of elements in our job list matches the number of elements in the job list we extracted using our simple IF statement.  But note the second method is better because it navigates through the hiearchy.  If the websites have multiple hiearchies with the word \"Python\" attached to it, I would be picking that up even though it may not be in the job description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Attributes From HTML Elements\n",
    "\n",
    "Next we will try to scrap the links from these job descriptions.  Note that there is a \"Learn\" and \"Apply\" link on each of the job card.  In HTML, all links are enclosed in ```<a href= >```.  We can try to extract them the same way as we extracted the job titles..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_element in python_job_elements:\n",
    "    # -- snip --\n",
    "    links = job_element.find_all(\"a\")  # Links all have an <a href = tag associated with them \n",
    "    for link in links:\n",
    "        print(link.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What? We don't actually get the links... Well this is because .text attribute only gives us the visible content, AKA the texts, of the HTML elements.  So first we need to extract all the ```<a>``` elements in the job card and then the href values using square-bracket notations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_element in python_job_elements:\n",
    "    # -- snip --\n",
    "    links = job_element.find_all(\"a\")  # Links all have an <a href = tag associated with them \n",
    "    for link in links:\n",
    "        link_url = link[\"href\"] # Extract link URL using [] reference\n",
    "        print(f\"Apply here: {link_url}\\n\") # This format prints link URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we also included \"Learn\" links.  If we only want \"Apply\" links we have to find a way to filter out the \"Learn\" links.  Luckily for us, we can add parameters to the ```.find_all()``` methods.  I did this by looking at the DOM and see what unique texts are contained in the \"Apply\" link.  Then I copied the **anonymous function** format above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_element in python_job_elements:\n",
    "    # -- snip --\n",
    "    links = job_element.find_all(\n",
    "        \"a\", string = lambda text:\"apply\" in text.lower() # Only look for \"a\" container with \"apply\" text contained in\n",
    "        ) \n",
    "    for link in links:\n",
    "        link_url = link[\"href\"] \n",
    "        print(f\"Apply here: {link_url}\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the method suggested in the tutorial.  We know that the \"Apply\" link is the second link that shows up in the DOM.  Since ```find_all()``` gives us a iterable, we can access the items in there similar to a list.  We just need to ```find_all``` the html elements with \"a\" in it, select the second item and the link, which is always behind a \"href\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is another way to do it\n",
    "\n",
    "for job_element in python_job_elements:\n",
    "    link_url = job_element.find_all(\"a\")[1][\"href\"] # Find the second link in the job_element container\n",
    "    print(f\"Apply here: {link_url}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Thank you for sticking with me if you are still here!  I really enjoyed following through this tutorial and I hope you do as well.  Just to sanity check, here is all the aspects of webscrapping and things we have accomplished so far:\n",
    "\n",
    "1. Retrieve HTML elements from URL using ```requests``` and parsing through them with ```BeautifulSoup```\n",
    "2. Creating iterable by using  ```.find_all()``` method to locate and extract HTML elements using **tags** and HTML **classes**\n",
    "3. Using ```.find()``` method to extract specific HTML elements using **IDs**\n",
    "4. Extracting text from HTML elements using ```.text.strip()``` methods\n",
    "5. Passing **anonymous functions** into ```.find_all()``` such as ```lambda``` that allow us to search for substrings\n",
    "6. Using ```.parent``` methods to access parent elements that encompasses all the information we want to extract\n",
    "7. Extracting **links** which have ```<a href =``` tag\n",
    "\n",
    "That being said, the below code is a condense version of hat we did above.  The code retrieves HTML elements from \"Fake Job\" website and extracts all the python jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries \n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Retrieve HTML elements from URL and parse \n",
    "URL = \"https://realpython.github.io/fake-jobs/\"\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "# Filter the hiearchy that encompasses all job cards using specific ID\n",
    "results = soup.find(id=\"ResultsContainer\")\n",
    "\n",
    "# Create iterable containing HTML elements with job title containing \"python\"\n",
    "python_jobs = results.find_all(\n",
    "    \"h2\", string = lambda text:\"python\" in text.lower() # tag_numer, string = lambda text: \"...\"\n",
    ")\n",
    "\n",
    "# Create a list to extract all levels of hiearchies in python_jobs \n",
    "python_job_elements = [\n",
    "    h2_elements.parent.parent.parent for h2_elements in python_jobs\n",
    "]\n",
    "print(len(python_job_elements)) # Check how many jobs selected\n",
    "\n",
    "# Print out all python related jobs and their links\n",
    "for job_element in python_job_elements:\n",
    "    # Print job title\n",
    "    title = job_element.find(\"h2\", class_ = \"title\")\n",
    "    print(title.text.strip())\n",
    "    # Print Company \n",
    "    company = job_element.find(\"h3\", class_ = \"company\") \n",
    "    print(company.text.strip())\n",
    "    # Print location\n",
    "    location = job_element.find(\"p\", class_ = \"location\")\n",
    "    print(location.text.strip())\n",
    "    # Print apply links\n",
    "    apply_link = job_element.find_all(\"a\")[1][\"href\"]\n",
    "    print(f\"Apply here: {apply_link}\\n\")\n",
    "\n",
    "\n",
    "# Damn, that's quite a lot! Congrats!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addtional Notes\n",
    "\n",
    "Query parameters in a URL consist of three main components:\n",
    "1. **Start symbol**: A question mark (?) denotes the beginning of the query parameters.\n",
    "2. **Information pairs**: Key-value pairs joined by an equal sign (key=value) hold the information.\n",
    "3. **Separator**: Multiple query parameters are separated by an ampersand symbol (&)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
