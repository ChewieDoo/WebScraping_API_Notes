{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping REMOTE.CO\n",
    "\n",
    "Hello!  This notebook walks through my thought process and web-scraping pipeline of the website [Remote Developer Jobs](https://remote.co/remote-jobs/developer).  Here I continue off from the Marin Breuss's [Beautiful Soup Tutoral](https://realpython.com/beautiful-soup-web-scraper-python/#find-elements-by-id) by practicing concepts I have learned on a suggested website. \n",
    "\n",
    "Overall I found this a bit more challenging because the HTML structure doesn't feel as \"organized\" as the Fake Python Job Website and I definately have to use a bit of my creativity that might be unconvetional to typical coding practice.  As always feel free to follow through and please provide any comments whether it'd be suggestion on coding etiquette, better methods, or pointing out any errors I have made.  I hope you enjoy going through this as much as I did!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up\n",
    "\n",
    "I use ```BeautifulSoup``` and ```requests``` in this excercise.  Our overall objective is the scrap all job-related content from the website and learn to filter some of the jobs we are interested in.\n",
    "\n",
    "Beautiful Soup Documentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "Data Source: https://remote.co/remote-jobs/developer/\n",
    "\n",
    "After inspecting the webpage, let's retrieve the HTML using URL provided and parse through the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://remote.co/remote-jobs/developer/\"\n",
    "page = requests.get(URL)\n",
    "\n",
    "soup = BeautifulSoup (page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After going through the DOM by inspecting the website, I identified the HTML element that encompasses all the job content by identifying it with the ```container tag``` and ```class```.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the elements of interest \n",
    "results = soup.find(\"div\", class_ = \"card bg-white m-0\")\n",
    "print(results.prettify()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure looks pretty solid to me.  Previously, I think I used a ```class``` parameter that is too generic so it picked up on a lot of the stuff we don't really need.\n",
    "\n",
    "### Scraping Job Contents\n",
    "\n",
    "Let's start scraping the job content.  I created a iterable ```job_cards``` containing all job related content by specifying unique parameters.  I used a FOR loop to iterate through all ```job_cards```, extracting job titles, company names, and apply links using ```.find()``` method.\n",
    "\n",
    "Notice here finding the company name is much more difficult.  The company name is a sub-text underneath each of the job card and is grouped together long with tags the job is tied with.  When I extracted the text from the HTML element, I also extracted the blank spaces and tags the job is associated with.  I had to use ```.split()``` method to turn the text into a list of strings and then index it to print out the correct text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create iterable containing all job postings\n",
    "job_cards = results.find_all(\"a\", class_ = \"card m-0 border-left-0 border-right-0 border-top-0 border-bottom\")\n",
    "\n",
    "for job in job_cards:\n",
    "    # Print out job title\n",
    "    title = job.find(\"span\", class_ = \"font-weight-bold larger\")\n",
    "    print(title.text.strip())\n",
    "    # Print out company \n",
    "    sub_text = job.find(\"p\", class_ = \"m-0 text-secondary\").text.strip() \n",
    "    sub_text_split = sub_text.split(\"\\n\") # Company name split from the rest of sub_text via a new line \"\\n\"\n",
    "    print(sub_text_split[0]) # Company name is first so print the first from the list of strings we splited\n",
    "    # Print job apply links\n",
    "    job_url = job[\"href\"]\n",
    "    print(f\"Apply: https://remote.co{job_url}\\n\") # Need to put the front of URL because it \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering specific jobs\n",
    "\n",
    "1. Selecting only full stack jobs\n",
    "2. Select only jobs that are high-paying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only \"Full Stack\" jobs\n",
    "\n",
    "# Create iterable containing all job posting with \"Full Stack\" in job title\n",
    "full_stack_jobs = results.find_all(\n",
    "    \"span\", class_ = \"font-weight-bold larger\", \n",
    "    string = lambda text: \"fullstack\" in text.lower() or # take into account different ways of writing full stack\n",
    "    \"full stack\" in text.lower() or \n",
    "    \"full-stack\" in text.lower()\n",
    "    )\n",
    "\n",
    "# List comprehension up the hiearchies to extract element that encompasses all content\n",
    "job_cards = [\n",
    "    elements.parent.parent.parent.parent.parent.parent for elements in full_stack_jobs\n",
    "]\n",
    "\n",
    "# Print out all job info\n",
    "for job in job_cards:\n",
    "    # Print out job title\n",
    "    title = job.find(\"span\", class_ = \"font-weight-bold larger\")\n",
    "    print(title.text.strip())\n",
    "    # Print out company - **note: see below code block why I chose to do this...\n",
    "    sub_text = job.find(\"p\", class_ = \"m-0 text-secondary\").text.strip() \n",
    "    sub_text_split = sub_text.split(\"\\n\") # Company name split from the rest of sub_text via \\n\n",
    "    print(sub_text_split[0]) # Company name is first so print the first from the list of strings we splited\n",
    "    # Print job apply links\n",
    "    job_url = job[\"href\"]\n",
    "    print(f\"Apply: https://remote.co{job_url}\\n\") # Need to put the front of URL because it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only jobs with high-paying tags\n",
    "high_pay_jobs = results.find_all(\n",
    "    \"small\", string = lambda text: \"high-paying\" in text.lower()\n",
    ")\n",
    "\n",
    "# Use list comprehension to select parent elements\n",
    "job_cards = [\n",
    "    elements.parent.parent.parent.parent.parent.parent.parent for elements in high_pay_jobs\n",
    "]\n",
    "\n",
    "# Print out all relevant info\n",
    "for job in job_cards:\n",
    "    title = job.find(\"span\", class_ = \"font-weight-bold larger\")\n",
    "    print(title.text.strip())\n",
    "    sub_text = job.find(\"p\", class_ = \"m-0 text-secondary\").text.strip()\n",
    "    sub_text_split = sub_text.split(\"\\n\")\n",
    "    print(sub_text_split[0])\n",
    "    job_url = job[\"href\"]\n",
    "    print(f\"Apply: https://remote.co{job_url}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome.  I hope everything worked.  Now we filtered out which jobs are high-paying so we know which jobs we should probably aim for if we wanna get that bag.  Thanks for following along!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
